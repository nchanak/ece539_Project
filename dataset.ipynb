{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funny python functions in other files don't get refreshed unless restart kernel, below magic is witchcraft that kinda fixes problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom script imports for compartmentalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:08:19.925305: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-15 23:08:19.952684: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-15 23:08:19.952730: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-15 23:08:19.952807: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-15 23:08:19.960426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-15 23:08:20.446722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import visualization, preprocess, metrics, naive_autoencoder, analyze_compression, wip_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if Tensorflow sees your GPU and has CUDA\n",
    "### Unnecessary unless you want to run large models on your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2801297251558259844\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 14211350528\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3413498496053723195\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "Built with CUDA: True\n",
      "Built with GPU support: True\n",
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:11:31.311618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-15 23:11:31.311677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-15 23:11:31.311698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-15 23:11:31.311850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-15 23:11:31.311861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-04-15 23:11:31.311882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-04-15 23:11:31.311895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 13553 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Built with GPU support:\", tf.test.is_built_with_gpu_support())\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model\n",
    "### Ensure when creating new model that you return the autoencoder itself as well as latent space encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 3, 64, 64, 3)]    0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 3, 64, 64, 32)    896       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 3, 64, 64, 64)    18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 3, 64, 64, 64)     295168    \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 3, 64, 64, 64)    36928     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 3, 64, 64, 32)    18464     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 3, 64, 64, 3)     867       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 370,819\n",
      "Trainable params: 370,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = naive_autoencoder.build_conv_lstm_autoencoder(\n",
    "    sequence_length=sequence_length,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PixelCNN input shape (T, H, W): (2, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define your input shape for the video sequences\n",
    "input_shape = (sequence_length, height, width, channels)\n",
    "\n",
    "# Build encoder and decoder\n",
    "encoder = wip_model.build_encoder(input_shape=input_shape, latent_channels=32)\n",
    "decoder = wip_model.build_decoder(latent_shape=encoder.output_shape[1:], output_channels=3)\n",
    "\n",
    "# Vector quantization layer\n",
    "vq_layer = wip_model.VectorQuantizer(num_embeddings=256, embedding_dim=32)\n",
    "\n",
    "# Get latent shape without embedding dimension for PixelCNNPrior input\n",
    "z_index_shape = encoder.output_shape[1:-1]  # (T, H, W)\n",
    "print(\"PixelCNN input shape (T, H, W):\", z_index_shape)\n",
    "\n",
    "# Build PixelCNN prior\n",
    "pixelcnn_prior = wip_model.PixelCNNPrior(input_shape=z_index_shape, num_embeddings=256)\n",
    "\n",
    "# Compose the full VQ-VAE module\n",
    "vqvae = wip_model.VQVAEModule(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    vq_layer=vq_layer,\n",
    "    pixelcnn_prior=pixelcnn_prior,\n",
    "    beta=1.0\n",
    ")\n",
    "\n",
    "# Compile with optimizer\n",
    "vqvae.compile(optimizer=tf.keras.optimizers.Adam(1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25089"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "# Clear TF session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "### Resizes, normalizes, and formats data-splitting into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (3128, 4, 64, 64, 3)\n",
      "Validation data shape: (782, 4, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "video_folder = \"extracted_videos\"\n",
    "\n",
    "# Preprocess the videos\n",
    "video_data, video_filenames = preprocess.preprocess_videos_with_mapping(\n",
    "    video_folder=video_folder,\n",
    "    sequence_length=sequence_length,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    channels=channels\n",
    ")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "split_index = int(0.8 * len(video_data))\n",
    "train_data = video_data[:split_index]\n",
    "val_data = video_data[split_index:]\n",
    "\n",
    "# Reshape the training and validation data to match the input shape of the autoencoder\n",
    "train_data = train_data.reshape((-1, sequence_length, height, width, channels))\n",
    "val_data = val_data.reshape((-1, sequence_length, height, width, channels))\n",
    "train_data = train_data.astype(np.float32)\n",
    "val_data = val_data.astype(np.float32)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)  # Expected: (num_samples, sequence_length, height, width, channels)\n",
    "print(\"Validation data shape:\", val_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for WIP model, not necessary to run currently\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def make_dataset(array, batch_size=4, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(array.astype(np.float32))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_dataset(train_data, batch_size=8)\n",
    "val_ds = make_dataset(val_data, batch_size=8, shuffle=False)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_data).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices(val_data).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONLY RUN IF YOU'RE NATHAN OR WANT TO WASTE TIME\n",
    "\n",
    "Or I suppose if you make a small baby model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 0.9669 - distortion: 0.4109 - rate: 0.5560 - val_loss: 1.0655 - val_distortion: 0.5410 - val_rate: 0.5245\n",
      "Epoch 2/7\n",
      "391/391 [==============================] - 18s 45ms/step - loss: 0.8106 - distortion: 0.3918 - rate: 0.4188 - val_loss: 1.2668 - val_distortion: 0.6076 - val_rate: 0.6592\n",
      "Epoch 3/7\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.6695 - distortion: 0.3727 - rate: 0.2969 - val_loss: 0.9258 - val_distortion: 0.4512 - val_rate: 0.4747\n",
      "Epoch 4/7\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.5867 - distortion: 0.3635 - rate: 0.2233 - val_loss: 0.7072 - val_distortion: 0.3852 - val_rate: 0.3220\n",
      "Epoch 5/7\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.5258 - distortion: 0.3513 - rate: 0.1745 - val_loss: 0.5922 - val_distortion: 0.3306 - val_rate: 0.2616\n",
      "Epoch 6/7\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.4692 - distortion: 0.3346 - rate: 0.1346 - val_loss: 0.5881 - val_distortion: 0.2894 - val_rate: 0.2987\n",
      "Epoch 7/7\n",
      "391/391 [==============================] - 18s 47ms/step - loss: 0.4274 - distortion: 0.3242 - rate: 0.1032 - val_loss: 0.5218 - val_distortion: 0.2873 - val_rate: 0.2345\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Output directory setup\n",
    "run_name = \"vqvae_video_compression\"\n",
    "output_dir = os.path.join(\"training_runs\", run_name)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(\n",
    "    filepath=os.path.join(output_dir, \"best_weights\"),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    "    ),\n",
    "    TensorBoard(log_dir=os.path.join(output_dir, \"logs\"))\n",
    "]\n",
    "\n",
    "history = vqvae.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=7,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (37146, 3, 64, 64, 3)\n",
      "Validation data shape: (9287, 3, 64, 64, 3)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 13:52:01.078876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2025-04-03 13:52:01.282855: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2025-04-03 13:52:01.282889: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2025-04-03 13:52:01.283137: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:317] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-04-03 13:52:01.815474: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fd9ee002930 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-04-03 13:52:01.815508: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti SUPER, Compute Capability 8.9\n",
      "2025-04-03 13:52:01.854659: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-03 13:52:02.078465: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:344] Couldn't read CUDA driver version.\n",
      "2025-04-03 13:52:02.109425: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9287/9287 [==============================] - 382s 41ms/step - loss: 7.7424e-04 - val_loss: 3.6359e-04\n",
      "Epoch 2/10\n",
      "9287/9287 [==============================] - 361s 39ms/step - loss: 1.4463e-04 - val_loss: 5.4172e-05\n",
      "Epoch 3/10\n",
      "9287/9287 [==============================] - 360s 39ms/step - loss: 9.0927e-05 - val_loss: 3.7964e-05\n",
      "Epoch 4/10\n",
      "9287/9287 [==============================] - 363s 39ms/step - loss: 7.0415e-05 - val_loss: 2.9867e-04\n",
      "Epoch 5/10\n",
      "9287/9287 [==============================] - 362s 39ms/step - loss: 6.1055e-05 - val_loss: 4.0588e-05\n",
      "Epoch 6/10\n",
      "9287/9287 [==============================] - 360s 39ms/step - loss: 5.1567e-05 - val_loss: 1.7583e-05\n",
      "Epoch 7/10\n",
      "9287/9287 [==============================] - 361s 39ms/step - loss: 4.6759e-05 - val_loss: 1.6843e-05\n",
      "Epoch 8/10\n",
      "9287/9287 [==============================] - 361s 39ms/step - loss: 4.2643e-05 - val_loss: 3.3355e-05\n",
      "Epoch 9/10\n",
      "9287/9287 [==============================] - 360s 39ms/step - loss: 3.9196e-05 - val_loss: 5.7956e-05\n",
      "Epoch 10/10\n",
      "9287/9287 [==============================] - 360s 39ms/step - loss: 3.3616e-05 - val_loss: 1.2270e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fde79ba0730>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the autoencoder\n",
    "autoencoder.fit(\n",
    "    train_data, train_data,\n",
    "    validation_data=(val_data, val_data),\n",
    "    epochs=10,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If have multiple autoencoders trained, can change name to load different one\n",
    "encoder_file = \"video_autoencoder.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVES ENCODER. ONLY USE IF YOU'VE JUST TRAINED A MODEL YOU WANT TO KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "autoencoder.save(encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Weights loaded from: training_runs/vqvae_video_compression/best_weights\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Loads the wip model weights\n",
    "weights_path = os.path.join(\"training_runs\", \"vqvae_video_compression\", \"best_weights\")\n",
    "vqvae.load_weights(weights_path)\n",
    "print(\"âœ“ Weights loaded from:\", weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADS MODEL, PROBABLY ALWAYS DO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "autoencoder = load_model(encoder_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's the metrics, wtf else is there to say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can get metrics for multiple random videos based on split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:00:25.602151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2025-04-15 23:00:25.668451: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2025-04-15 23:00:25.668478: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2025-04-15 23:00:25.668514: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:25.751164: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2025-04-15 23:00:25.820554: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/99 [========>.....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:00:26.090365: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 1s 5ms/step\n",
      "ðŸ“¹ Evaluating: celebv_-_Bdf9C0SAU_8.mp4 â€” 99 sequences\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 23:00:27.268343: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.268467: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.268542: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.268625: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.269854: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.269883: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.269918: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.271277: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.385460: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.475523: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.475570: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.475605: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.476481: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.476528: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.484416: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.492075: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2025-04-15 23:00:27.499749: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 1s 5ms/step\n",
      "ðŸ“¹ Evaluating: celebv_-_Bdf9C0SAU_3.mp4 â€” 121 sequences\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "94/94 [==============================] - 0s 5ms/step\n",
      "ðŸ“¹ Evaluating: celebv_-_Bdf9C0SAU_9.mp4 â€” 94 sequences\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/nchanak/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'celebv_-_Bdf9C0SAU_8.mp4': {'psnr': 26.858852,\n",
       "  'ssim': 0.6907404,\n",
       "  'lpips': 0.23208558992153466},\n",
       " 'celebv_-_Bdf9C0SAU_3.mp4': {'psnr': 27.105642,\n",
       "  'ssim': 0.71718764,\n",
       "  'lpips': 0.2303910284680276},\n",
       " 'celebv_-_Bdf9C0SAU_9.mp4': {'psnr': 25.978357,\n",
       "  'ssim': 0.7215823,\n",
       "  'lpips': 0.22211066344158448}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_val = metrics.evaluate_metrics_by_video(\n",
    "    split=\"val\",\n",
    "    limit=3,\n",
    "    video_data=video_data,\n",
    "    video_filenames=video_filenames,\n",
    "    autoencoder=vqvae,\n",
    "    sequence_length=sequence_length,\n",
    "    height=height,\n",
    "    width=width\n",
    ")\n",
    "metrics_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression\n",
    "### Yes very bad compression rn cuz latent space huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[â–¶] Processing 83 sequences from: celebv_-_Bdf9C0SAU_0.mp4\n",
      "83/83 [==============================] - 0s 5ms/step\n",
      "83/83 [==============================] - 0s 5ms/step\n",
      "\n",
      "Compression Results for 'celebv_-_Bdf9C0SAU_0.mp4':\n",
      "Raw 64x64 input size:    438.18 KB\n",
      "Latent compressed size:  151012.24 KB\n",
      "Estimated compression:   0.00x\n",
      "Deleted: output_latent.npz\n",
      "Deleted: output_original_input.npz\n",
      "Deleted: output_reconstructed.npz\n"
     ]
    }
   ],
   "source": [
    "import analyze_compression\n",
    "\n",
    "# Assume you've already run\n",
    "# autoencoder, encoder = build_conv_lstm_autoencoder(...)\n",
    "# autoencoder.load_weights(...)\n",
    "# video_data, video_filenames = preprocess_videos_with_mapping(...)\n",
    "\n",
    "result = analyze_compression.analyze_compression_from_video_file(\n",
    "    video_name=\"celebv_-_Bdf9C0SAU_0.mp4\",\n",
    "    autoencoder=autoencoder,\n",
    "    encoder=encoder,\n",
    "    sequence_length=3,\n",
    "    height=64,\n",
    "    width=64,\n",
    "    cleanup=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating compression on 3 val videos...\n",
      "\n",
      "99/99 [==============================] - 3s 27ms/step\n",
      "267/267 [==============================] - 7s 26ms/step\n",
      "94/94 [==============================] - 2s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'celebv_-_Bdf9C0SAU_8.mp4': {'video': 'celebv_-_Bdf9C0SAU_8.mp4',\n",
       "  'num_sequences': 99,\n",
       "  'num_frames': 396,\n",
       "  'input_kb_estimated': 3282.0314071120415,\n",
       "  'codec_kb': 36.412109375,\n",
       "  'latent_kb_estimated': 13.292874336242676,\n",
       "  'compression_ratio_codec': 90.13571208718368,\n",
       "  'compression_ratio_latent': 246.90155974495812,\n",
       "  'vq_time_secs': 3.647674322128296},\n",
       " 'celebv_-_Bdf9C0SAU_3.mp4': {'video': 'celebv_-_Bdf9C0SAU_3.mp4',\n",
       "  'num_sequences': 267,\n",
       "  'num_frames': 1068,\n",
       "  'input_kb_estimated': 9001.780255015248,\n",
       "  'codec_kb': 134.998046875,\n",
       "  'latent_kb_estimated': 34.84114074707031,\n",
       "  'compression_ratio_codec': 66.68081845176879,\n",
       "  'compression_ratio_latent': 258.3664042565019,\n",
       "  'vq_time_secs': 8.400297164916992},\n",
       " 'celebv_-_Bdf9C0SAU_9.mp4': {'video': 'celebv_-_Bdf9C0SAU_9.mp4',\n",
       "  'num_sequences': 94,\n",
       "  'num_frames': 376,\n",
       "  'input_kb_estimated': 3259.8387960985624,\n",
       "  'codec_kb': 31.8603515625,\n",
       "  'latent_kb_estimated': 12.690022468566895,\n",
       "  'compression_ratio_codec': 102.31647286451886,\n",
       "  'compression_ratio_latent': 256.88203501397754,\n",
       "  'vq_time_secs': 2.8092384338378906}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import analyze_compression\n",
    "\n",
    "importlib.reload(analyze_compression)\n",
    "with tf.device('/CPU:0'):\n",
    "    compression_results = analyze_compression.evaluate_compression_by_video(\n",
    "        split=\"val\",\n",
    "        limit=3,\n",
    "        video_data=video_data,\n",
    "        video_filenames=video_filenames,\n",
    "        autoencoder=vqvae,\n",
    "        encoder=encoder,\n",
    "        sequence_length=sequence_length,\n",
    "        height=height,\n",
    "        width=width\n",
    "    )\n",
    "compression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires: conda install -c conda-forge ffmpeg\n",
    "\n",
    "alternatively: sudo apt install ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 187ms/step\n",
      "[âœ“] Comparison saved: comparison_celebv_-_Bdf9C0SAU_6.avi\n",
      "[â–¶] Converting comparison_celebv_-_Bdf9C0SAU_6.avi â†’ comparisons/comparison_celebv_-_Bdf9C0SAU_6.mp4\n",
      "Deleted: comparison_celebv_-_Bdf9C0SAU_6.avi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, avi, from 'comparison_celebv_-_Bdf9C0SAU_6.avi':\n",
      "  Metadata:\n",
      "    software        : Lavf59.27.100\n",
      "  Duration: 00:00:03.92, start: 0.000000, bitrate: 52 kb/s\n",
      "  Stream #0:0: Video: mpeg4 (Simple Profile) (XVID / 0x44495658), yuv420p, 128x64 [SAR 1:1 DAR 2:1], 24 fps, 24 tbr, 24 tbn, 24 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55a5bff921c0] using SAR=1/1\n",
      "[libx264 @ 0x55a5bff921c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x55a5bff921c0] profile High, level 2.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55a5bff921c0] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=8 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'comparisons/comparison_celebv_-_Bdf9C0SAU_6.mp4':\n",
      "  Metadata:\n",
      "    software        : Lavf59.27.100\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 512x256 [SAR 1:1 DAR 2:1], q=2-31, 24 fps, 12288 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=   94 fps=0.0 q=-1.0 Lsize=      80kB time=00:00:03.79 bitrate= 173.2kbits/s speed=  29x    \n",
      "video:78kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 2.502591%\n",
      "[libx264 @ 0x55a5bff921c0] frame I:1     Avg QP:21.20  size:  3054\n",
      "[libx264 @ 0x55a5bff921c0] frame P:24    Avg QP:21.38  size:  1841\n",
      "[libx264 @ 0x55a5bff921c0] frame B:69    Avg QP:23.95  size:   466\n",
      "[libx264 @ 0x55a5bff921c0] consecutive B-frames:  2.1%  0.0%  0.0% 97.9%\n",
      "[libx264 @ 0x55a5bff921c0] mb I  I16..4: 20.1% 77.5%  2.3%\n",
      "[libx264 @ 0x55a5bff921c0] mb P  I16..4:  6.6% 13.6%  0.2%  P16..4: 43.4% 10.6%  3.6%  0.0%  0.0%    skip:21.8%\n",
      "[libx264 @ 0x55a5bff921c0] mb B  I16..4:  0.2%  0.3%  0.0%  B16..8: 33.2%  2.1%  0.1%  direct: 1.9%  skip:62.2%  L0:53.3% L1:42.9% BI: 3.9%\n",
      "[libx264 @ 0x55a5bff921c0] 8x8 transform intra:68.3% inter:95.1%\n",
      "[libx264 @ 0x55a5bff921c0] coded y,uvDC,uvAC intra: 40.8% 63.1% 6.5% inter: 8.6% 19.1% 0.0%\n",
      "[libx264 @ 0x55a5bff921c0] i16 v,h,dc,p: 47% 18%  8% 27%\n",
      "[libx264 @ 0x55a5bff921c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 35% 16% 16%  2%  7% 10%  6%  4%  3%\n",
      "[libx264 @ 0x55a5bff921c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 34% 18% 18%  2%  8% 10%  5%  3%  2%\n",
      "[libx264 @ 0x55a5bff921c0] i8c dc,h,v,p: 45% 10% 35% 10%\n",
      "[libx264 @ 0x55a5bff921c0] Weighted P-Frames: Y:4.2% UV:0.0%\n",
      "[libx264 @ 0x55a5bff921c0] ref P L0: 57.8%  9.0% 21.6% 11.5%  0.1%\n",
      "[libx264 @ 0x55a5bff921c0] ref B L0: 83.4% 12.8%  3.8%\n",
      "[libx264 @ 0x55a5bff921c0] ref B L1: 95.3%  4.7%\n",
      "[libx264 @ 0x55a5bff921c0] kb/s:162.15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"512\" height=\"256\" controls>\n",
       "      <source src=\"comparisons/comparison_celebv_-_Bdf9C0SAU_6.mp4\" type=\"video/mp4\">\n",
       "      Your browser does not support the video tag.\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_video = \"celebv_-_Bdf9C0SAU_6.mp4\"\n",
    "\n",
    "visualization.generate_and_display_comparison(\n",
    "    video_name=play_video,\n",
    "    autoencoder=vqvae,\n",
    "    sequence_length=sequence_length,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    cleanup=True \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU (WSL)",
   "language": "python",
   "name": "tf-gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
